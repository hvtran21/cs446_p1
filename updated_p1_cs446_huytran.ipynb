{"cells":[{"cell_type":"markdown","metadata":{"id":"ZHj92VDfTEPO"},"source":["Your name: Huy Tran\n","\n","Your student ID number: 33259590\n","\n","Shared link to this notebook: https://github.com/hvtran21/cs446_p1"]},{"cell_type":"markdown","metadata":{"id":"HoFerkqpTEPQ"},"source":["## Programming Assignment 1 (P1) for COMPSCI 446, Search Engines"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"hWAYQYm7AiFQ"},"outputs":[],"source":["version = 3 # DO NOT MODIFY. If notebook does not match with autograder version, the tests are likely fail!!"]},{"cell_type":"markdown","metadata":{"id":"sVSv_Zs7TEPR"},"source":["The purpose of this project is to explore tokenization, stopword removal, and stemming within a few real documents, and to investigate term statistics as related to Heaps' and possibly Zipf's Laws. Although actual time to complete this project will vary widely across students, expect to spend several hours if you are a strong programmer and 10+ hours if you're a bit rusty.\n","\n","The description below is quite lengthy and will probably feel complicated at first, but each portion is just one piece of a sequence of rules that you need to apply to the tokens in the file. At a high level, the program will do the following:\n","* It will read in the (gzip compressed) input file and break it into tokens (loosely speaking, \"words\") using spaces or a fancier approach.\n","* It will optionally remove tokens that match stopwords in a provided list.\n","* It will optionally stem those tokens to their root form using a simple suffix-s stemmer or the Porter stemmer.\n","* It will calculate some statistics and summary information about the resulting tokens.\n","* It will generate incremental and cumulative information about the tokens the system encounters and generates."]},{"cell_type":"markdown","metadata":{"id":"EQ5lzWGuTEPS"},"source":["Here are a list of provided files that will be loaded into your Google Drive for use in this notebook:\n","\n","* _P1-train.gz_, a compressed file that you can use as input to your program.\n","\n","* _PandP.gz_, which is a much larger text file for you to play with, though we will not provide any sample output. This is the text of Jane Austen's novel, _Pride and Prejudice_, downloaded from Project Gutenberg. You can see the original at https://www.gutenberg.org/ebooks/1342; what is provided below is the UTF-8 version, but with leading and trailing material removed, and with some unusual characters converted to ASCII equivalents (e.g., smart quotation marks, the British pound symbol).\n","\n","* _stopwords.txt_ is a file containing the stopwords you will use in the stopping step.\n","\n","\n","Note, after the gradescope autograder is configured (**it is not ready yet**!), uploading the notebook file to gradescope will show the result on sample inputs. Please make sure the notebook is compilable (not only locally to your computer, but also on gradescope)."]},{"cell_type":"markdown","metadata":{"id":"cgnbrPK87DIS"},"source":["We first execute the following to connect to Google Drive (you will be prompted repeatedly for access to your Google Drive; please give it permission) and download copies of the sample files listed above. You should not need to make any modifications to the code, though if you want to use a slightly different path in Google Drive, you can modify the appropriate data_path value. (The autograder will not use your Google Drive.)"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":24919,"status":"ok","timestamp":1727551240638,"user":{"displayName":"Chang Zeng","userId":"16004812550788429459"},"user_tz":240},"id":"ZjCDAZdyTEPT","outputId":"510356f2-c4c3-4365-f83a-7b60aa96b39c"},"outputs":[],"source":["import os\n","import string\n","import gzip\n","import re\n","\n","from collections import Counter\n","\n","try:\n","    from google.colab import drive\n","    in_colab = True\n","except ImportError:\n","    in_colab = False\n","\n","\n","# You are more than welcome to code some helper functions.\n","# But do note that we are only grading functions that are coded in the template files.\n","\n","\n","# Connect to Google Drive and download copies of the sample files listed above.\n","# Please allow the access to your Google Drive or the following dataset loader will fail.\n","# (The autograder will not use your Google Drive.)\n","if in_colab:\n","  drive.mount(\"/content/drive/\") ## DO NOT MODIFY THIS LINE\n","  data_path = \"/content/drive/MyDrive/CS 446 Search Engines\" ## CHANGE TO YOUR OWN FOLDER ON GOOGLE DRIVE\n","else:\n","  data_path = \"./data/\"  ## DO NOT MODIFY THIS LINE. CHANGING THIS LINE WOULD RESULT IN FAIL OF AUTOGRADER TESTS"]},{"cell_type":"markdown","metadata":{"id":"FixeP9rOTEPT"},"source":["### 0. Pre-processing\n","\n","We will now load the texts that stored at <em>data_path</em> so that you can work with them in the rest of the notebook. We will not bother loading the files if they have already been loaded to your Google Drive, so this should be a one-time effort.\n","\n","**NOTE**: This code will not create a new folder for you. It assumes that you have a folder called COMPSCI446 at the top level of your Google Drive. You can create that folder and everything will work fine. If you want to create something different (e.g., COMPSCI446-P1 or COMPSCI446/P1) then create that folder and edit the data_path line in the cell above.\n","\n","This next step also loads the files into lists of strings so that you can use them directly elsewhere. That is, you do not need to open and read in the files yourself: it's done.\n","\n","Note, in this assignment, to practice the tokenization process, we load the entire file directly into memory and then process it line by line. However, in the real-world scenario, when dealing with large dataset (saying billions of entries), it would be better to process the content line by line to save memory of the machine and/or because the whole file wouldn't even fit in memory."]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1441,"status":"ok","timestamp":1727551242077,"user":{"displayName":"Chang Zeng","userId":"16004812550788429459"},"user_tz":240},"id":"uKl-5NrlTEPU","outputId":"fd06bf45-7eb0-4312-dfc2-68273018024e"},"outputs":[{"name":"stdout","output_type":"stream","text":["File \"PandP.gz\" already exists, not downloading.\n","File \"P1-train.gz\" already exists, not downloading.\n","File \"stopwords.txt\" already exists, not downloading.\n","First line of         PandP.gz:                          [Illustration:\n","First line of      P1-train.gz: First, here are all of the stopwords, none of which should make it through if yesStop is selected.\n","First line of    stopwords.txt: a\n"]}],"source":["import urllib.request\n","import gzip\n","from pathlib import Path\n","\n","def load_file(file_path: str, gz_zip: bool = True) -> list[str]:\n","    \"\"\"\n","    Load strings from the text or gzip file. Remember to strip newline if necessary!\n","\n","    Args:\n","        file_path: the location of file we want to analyze on.\n","        gz_zip: true if the file is gzipped, false otherwise\n","\n","    Returns: an array of string loaded from the text or gzip file.\n","    \"\"\"\n","    webloc = \"https://cs.umass.edu/~allan/cs446/\"\n","\n","\n","    data_info = Path(data_path)\n","    if not data_info.exists() or not data_info.is_dir():\n","      print(f\"Google folder \\\"{data_path}\\\" is not present or not a folder. Nothing will work from here.\")\n","      return []\n","\n","    local_google_drive_path = os.path.join(data_path,file_path)\n","    local_file = Path(local_google_drive_path)\n","    if local_file.is_file():\n","        print(f\"File \\\"{file_path}\\\" already exists, not downloading.\")\n","    else:\n","        print(f\"Cannot find \\\"{file_path}\\\" so downloading it\")\n","        urllib.request.urlretrieve(webloc + file_path, local_google_drive_path)\n","        print(\"Done\")\n","\n","    if not gz_zip:\n","        f = open(local_google_drive_path, \"r\", encoding=\"utf-8-sig\")\n","    else:\n","        # Read compressed file (opened in text mode since that's what we use)\n","        f = gzip.open(local_google_drive_path, \"rt\", encoding=\"utf-8-sig\")\n","    results = [line.strip(\"\\n\") for line in f.readlines() if line]\n","\n","    if f:\n","      f.close()\n","\n","    return results\n","\n","\n","\n","# path to documents and stopwords\n","documents_path = \"PandP.gz\" # path to gzip file that contains documents\n","train_data_path = \"P1-train.gz\" # Path to p1 train dataset\n","stopwords_path = \"stopwords.txt\"  # path to the file that contains stopwords\n","\n","sentences = load_file(documents_path)\n","train_sentences = load_file(train_data_path)\n","stopwords = load_file(stopwords_path, gz_zip = False)\n","print(f\"First line of {documents_path:>16}: {sentences[0]}\");\n","print(f\"First line of {train_data_path:>16}: {train_sentences[0]}\");\n","print(f\"First line of {stopwords_path:>16}: {stopwords[0]}\");"]},{"cell_type":"markdown","metadata":{"id":"DCL5mLymTEPU"},"source":["### 1. Tokenization\n","\n","The first step of processing text is to break it into tokens. That will be either by using whitespace or two different fancier process. How you do that will be determined by the choice of tokenization function as follows.\n","\n","Each of these functions accepts a string (in this case, a line/sentence from one of the input files) and will produce a list of tokens as strings. In all cases, the tokens should preserve their order within original documents and if extra tokens are generated, they should be in the corresponding position of the token that triggered their creation.\n","\n","You can see how these (and other functions) are used by looking at the tokenization() function in section 4.1. Note that the autograder expects these functions to have the arguments as listed here."]},{"cell_type":"markdown","metadata":{"id":"ZUKVrr66TEPV"},"source":["#### 1.1 <u>tokenize_space()</u>\n","\n","This tokenizer will break the input line into a list of whitespace-separated tokens. If there are multiple whitespace characters in a row (multiple spaces, a space and a tab, a space and a line break, etc.) they are treated as a single token break: e.g., \"a&lt;space&gt;&lt;space&gt;b\" is two tokens (\"a\" and \"b\") rather than three (\"a\", &lt;empty&gt;, \"b\"). Do not make any other changes to the tokens beyond separating them. So, for example, the start of this paragraph would result in the following 16 tokens (for the sake of space, listed across the line separated by ⍟, though that is not what you will actually do in your output):\n","\n","    This ⍟ tokenizer ⍟ will ⍟ break ⍟ the ⍟ file ⍟ into ⍟ a ⍟ list ⍟ of ⍟ whitespace-separated ⍟ tokens.\n","\n","Note that punctuation is included with a token based on where the whitespace is and tokens are produced without changing their case. There are very simple solutions to this: you can do this in one line."]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17,"status":"ok","timestamp":1727551242077,"user":{"displayName":"Chang Zeng","userId":"16004812550788429459"},"user_tz":240},"id":"Tt32gDp1TEPW","outputId":"ac71b60f-c5df-48d7-803d-0d656a0b9651"},"outputs":[{"name":"stdout","output_type":"stream","text":["Input string: \"First, here are all of the stopwords, none of which should make it through if yesStop is selected.\"\n","['First,', 'here', 'are', 'all', 'of', 'the', 'stopwords,', 'none', 'of', 'which', 'should', 'make', 'it', 'through', 'if', 'yesStop', 'is', 'selected.']\n"]}],"source":["def tokenize_space(input_str: str) -> list[str]:\n","    \"\"\"\n","    Tokenize sentence into whitespace-separated tokens. For\n","    example, input_str = \"2024 Fall Search Engine\", the function\n","    should return [\"2024\", \"Fall\", \"Search\", \"Engine\"].\n","\n","    Args:\n","        input_str: the sentence/phrase that we want to tokenize.\n","\n","    Returns: an array of whitespace-separated tokens.\n","    \"\"\"\n","\n","    #########\n","    ##\n","    ## Implement the function here\n","    ##\n","    #########\n","    input_str = re.sub(r\"\\s+\", \" \", input_str.replace(\"\\n\", \" \").replace(\"\\t\", \" \"))\n","    return input_str.split()\n","\n","tokens_separated_by_space = tokenize_space(train_sentences[0])\n","print( f\"Input string: \\\"{train_sentences[0]}\\\"\" )\n","print(tokens_separated_by_space)"]},{"cell_type":"markdown","metadata":{"id":"G0dJ6_27yacd"},"source":["#### 1.2 tokenize_4grams()\n","\n","This is another straightforward tokenizer, but one that does have as many ready solutions on hand. Unlike the one above, this one _treats spaces as part of the token_. Every four characters is a token, regardless of whether the characters are whitespace, punctuation, or alphanumeric characters. The one exception so that is that you should treat any white-space type character (newline or tab) as if it were a space. _In addition_, if there are multiple whitespace characters in a row (multiple spaces, a space and a tab, a space and a line break, etc.) they are treated as a single space: e.g., \"a&lt;space&gt;&lt;space\\>bc\" is a single token (\"a&lt;space&gt;bc\") rather than the token \"a&lt;space&gt;&lt;space&gt;b\" and so on). Similarly, \"a&lt;space&gt;&lt;newline&gt;&lt;tab&gt;bcef\" should generate \"a&lt;space&gt;bc\" and \"bcef\".  \n","\n","It turns out that in most applications we need to use \"overlapping\" n-grams. So you should start each token two characters after the previous one. Using the same notation as above, the sequence\n","\n","    An  iced coffee \\t\n","    is very nice\n","\n","would generate the following sequence of tokens (using the same notation as above, with ⍟ showing where tokens break). Spaces are replaced with underscores so they are easier to see:\n","\n","    An_i ⍟ _ice ⍟ ced_ ⍟ d_co ⍟ coff ⍟ ffee ⍟ ee_i ⍟ _is_ ⍟ s_ve ⍟ very ⍟ ry_n ⍟ _nic ⍟ ice\n","\n","For the final token, do not include the 1-, 2-gram that ends that final token. And if the number of characters in the file is not a multiple of four, stop with the final 3-gram (like what we have in the example with _ice_ but not also _e_).\n","\n","In special cases, if the string is too short to create even one 4-gram, you should return the original token instead."]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17,"status":"ok","timestamp":1727551242078,"user":{"displayName":"Chang Zeng","userId":"16004812550788429459"},"user_tz":240},"id":"oz_aNKqX0kJX","outputId":"13558b39-dcc7-4880-e8b6-1bdd83c60ffc"},"outputs":[{"name":"stdout","output_type":"stream","text":["An_i ⍟ _ice ⍟ ced_ ⍟ d_co ⍟ coff ⍟ ffee ⍟ ee_i ⍟ _is_ ⍟ s_ve ⍟ very ⍟ ry_n ⍟ _nic ⍟ ice\n","_\n","Firs ⍟ rst, ⍟ t,_h ⍟ _her ⍟ ere_ ⍟ e_ar ⍟ are_ ⍟ e_al ⍟ all_ ⍟ l_of ⍟ of_t ⍟ _the ⍟ he_s ⍟ _sto ⍟ topw ⍟ pwor ⍟ ords ⍟ ds,_ ⍟ ,_no ⍟ none ⍟ ne_o ⍟ _of_ ⍟ f_wh ⍟ whic ⍟ ich_ ⍟ h_sh ⍟ shou ⍟ ould ⍟ ld_m ⍟ _mak ⍟ ake_ ⍟ e_it ⍟ it_t ⍟ _thr ⍟ hrou ⍟ ough ⍟ gh_i ⍟ _if_ ⍟ f_ye ⍟ yesS ⍟ sSto ⍟ top_ ⍟ p_is ⍟ is_s ⍟ _sel ⍟ elec ⍟ ecte ⍟ ted.\n","Fina ⍟ nall ⍟ lly, ⍟ y,_t ⍟ _the ⍟ he_r ⍟ _res ⍟ est_ ⍟ t_of ⍟ of_t ⍟ _the ⍟ he_t ⍟ _tex ⍟ ext_ ⍟ t_is ⍟ is_d ⍟ _des ⍟ esig ⍟ igne ⍟ ned_ ⍟ d_to ⍟ to_s ⍟ _str ⍟ tres ⍟ ess_ ⍟ s_yo ⍟ your ⍟ ur_f ⍟ _fan ⍟ ancy ⍟ cy_t ⍟ _tok ⍟ oken ⍟ eniz ⍟ izer ⍟ er._\n"]}],"source":["import re\n","def tokenize_4grams(input_str: str) -> list[str]:\n","    \"\"\"\n","    Tokenize sentence into 4char tokens with shifting windows. For example,\n","    input_str = \"An\\ticed coffee \\t\\nis very nice\", the function should return\n","    ['An i', ' ice', 'ced ', 'd co', 'coff', 'ffee', 'ee i', ' is ', 's ve', 'very', 'ry n', ' nic', 'ice'].\n","\n","    Args:\n","        input_str: the sentence/phrase that we want to tokenize.\n","\n","    Returns: an array of 4-grams tokens.\n","    \"\"\"\n","\n","    #########\n","    ##\n","    ## Implement the function here\n","    ##\n","    #########\n","    if input_str == \"\":\n","        return [input_str]\n","    \n","    input_str = re.sub(r\"\\s+\", \" \", input_str.replace(\"\\n\", \" \").replace(\"\\t\", \" \"))\n","\n","    if len(input_str) == 1:\n","        return input_str\n","    \n","    return [input_str[i:i+4] for i in range(0, len(input_str)-2, 2)]\n","\n","token_4grams = tokenize_4grams(sentences[0])\n","\n","# Check if the output matches the example mentioned above\n","print(' ⍟ '.join([elem.replace(' ', '_') for elem in tokenize_4grams(\"An\\ticed coffee \\t\\nis very nice\")]))\n","print(' ⍟ '.join([elem.replace(' ', '_') for elem in tokenize_4grams(\"    \")]))\n","print(' ⍟ '.join([elem.replace(' ', '_') for elem in tokenize_4grams(\"First, here are all of the stopwords, none of which should make it through if yesStop is selected.\")]))\n","print(' ⍟ '.join([elem.replace(' ', '_') for elem in tokenize_4grams(\"Finally, the rest of the text is designed to stress your fancy tokenizer. \")]))\n"]},{"cell_type":"markdown","metadata":{"id":"YinFt0HSTEPX"},"source":["#### 1.2 <u>tokenize_fancy()</u>\n","\n","This is a more complicated tokenizer that builds on the _spaces_ tokenizer (not the 4-gram version), applying the following additional rules. You can implement these rules in any way that makes sense to you as long as you honor the result of the rules as presented below, including dependencies between the rules. They are described on the assumption that you apply them as a sequence of steps in the order listed, though, and encourage you to do that rather than be clever.\n","\n","*Hint: some of the steps generate new sequences that, in turn, need to be tokenized. You will probably want to think about a way to use recursion to handle that.*\n","\n","1. Start with **ONE** space-separated token as in the <em>spaces </em>tokenizer above. (In reality, you'll do this for all of them. For simplicity we consider one at a time.)\n","\n","1. A token that is a URL of the form \"https://&hellip;\" or \"http://&hellip;\" should be recognized as a single token. If there is trailing punctuation, it is not part of the URL and that punctuation should be discarded. Note that \"https://hithere.com+https://howdy.org\" should be considered as one simple token (since it starts with https). Also note that \"446https://hithere.com\" is _not_ a URL because it does not start with the URL signal. Make sure to handle the cases of \"HTTP\" and \"HTTPS\" (etc.) as the start of a url is case-insensitive. No other changes to a URL token should happen &ndash; i.e., for a URL, ignore the remaining rules below.\n","\n","3. Convert the tokens to lowercase.\n","  \n","4. Treat numbers as a single token, including plus and/or minus signs, commas, and decimal points if they exist, so that \"3,141.59\" remains that string. A number can only be a sequence of numeric characters and those punctuation marks indicated <span class=\"\" style=\"color: #98ca3e;\">but it must have at least one number character. You do not have to validate that the number is well formed (so \"3+14..1,59\" and \"-1-5\" would both still be number tokens). Do no other processing to number tokens.\n","\n","5. Apostrophes should be \"squeezed out\" so that it is as if they were never there. That means that a contraction such as \"don't\" would turn into \"dont\" and a name like \"O'Brian\" will become \"obrian\" (because it is was also lowercased in step 3).\n","\n","6. Any remaining punctuation <u>other than periods and hyphens</u> should be treated as a word separator (as if it were white space: note the sequence of whitespace rules from the spaces tokenizer), except recall that punctuation in a URL (step 2) or number (step 4) is not a word break. So one token can generate multiple tokens: \"3/11//23\" will result in [\"3\", \"11\", \"23\"], and \"3^rd\" will result in [\"3\", \"rd\"], but \"3.14/pi\" will result in [\"3.14\", \"pi\"]. If a new token is a number (as defined above), do not process it further. So in the example, \"3.14\" will not be treated as an abbreviation by step 8. You may prefer to do this recursively to handle these rules on sub-parts. So that \"b.c.,\" (note the comma) would result in \"b.c.\" (remember that periods are not word breaks) and an empty token. A recursive call on \"b.c.\" will result in \"bc\" from abbreviation rule 8 and the empty token will be discarded.\n","\n","7. Hyphens (not treated as word separators) within a token should be processed in three steps: (1) remove the original token with its hyphen(s), (2) add new tokens with the hyphens treated as space separators (as in the spaces tokenizer), and then (3) add a new token with all hyphens squeezed out. So,\n","\n","  [\"data-base\"] &rarr; [\"data\", \"base\", \"database\"]<br/>\n","  [\"mother-in-law\"] &rarr; [\"mother\", \"in\", \"law\", \"motherinlaw\"]<br/>\n","  [\"a.-b.-c.\"] &rarr; [\"a.\", \"b.\", \"c.\", \"a.b.c.\"]\n","\n","  Note that as with the punctuation rule, the hyphen processing will result in additional tokens. Apply all tokenization rules to each of the tokens that are generated. So \"1.-on-1.\" will generate [\"1.\", \"on\", \"1.\", \"1.on.1.\"]. The occurrences of \"1.\" will remain unchanged because they are numbers, but \"1.on.1.\" will be treated as an abbreviation and converted into \"1on1\". There are some edge cases that will cause odd things to happen (e.g., the token \"1-https://Some.Url\" will end up converting the embedded URL to lowercase which contractadicts rule 2), but we won't stress about unusual subtokens caused by hyphens; we'll just accept and respect them for being what they are.\n","\n","8. Treat abbreviations as a single token. If a token is not a number or URL and the only punctuation it contains is periods, it is an abbreviation. Note that this includes a token that comprises nothing but periods, even if that is not intuitive. For an abbreviation, remove all periods. So \"i.b.m.\" converts to \"ibm\" and \"Ph.D.\" and \"Ph.D\" both go to \"phd\" (because an abbreviation does not need to end with a period and because rule 3 converted it to lowercase). If the result of removing the periods is an empty string, then treat it as an empty token and do not report it out."]},{"cell_type":"markdown","metadata":{"id":"DjVPROnPTEPY"},"source":["Here are some sample tokenizations that should happen:\n","* Token &rarr; token (lower case rule)\n","* She's &rarr; shes (apostrophe rule and lower case rule)\n","* Mother's-IN-Law &rarr; mothers, in, law, mothersinlaw (apostrophe, lowercase, hyphens)\n","* U.mass &rarr; umass (lowercase, abbreviation)\n","* go!!!!team &rarr; go team (non-period punctuation rule)\n","* USD\\$10.30 &rarr; usd 10.30 (non-period punctuation, case, number)\n","* USD\\$10,30 &rarr; usd 10 30 (not period punctuation, case)\n","* USD\\$10-30 &rarr; usd 10-30 (not-period punctuation, number (not hyphen!!))\n","\n","There will be interactions between rules, and that is OK. For example,\n","* [\"a.-2./c.\"] &rarr; [\"a.-2.\", \"c.\"] by non-period and non-hyphen punctuation rule\n","* &rarr; [\"a.\", \"2.\", \"a.2.\", \"c.\"] by hyphen rule\n","* &rarr; [\"a\", \"2.\", \"a2\", \"c\"] after applying the abbreviation rule, noting that \"2.\" is a number so retains its decimal point.\n","\n","You may use a regular expression library to help identify patterns, but other third party libraries are <em>not </em>allowed for this part. You may, of course, use the standard libraries in Python."]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16,"status":"ok","timestamp":1727551242078,"user":{"displayName":"Chang Zeng","userId":"16004812550788429459"},"user_tz":240},"id":"j-hfI34q7Qd0","outputId":"0ea05849-e60c-43e8-dd8e-701f16e12d0f"},"outputs":[{"name":"stdout","output_type":"stream","text":["a.-2./c. ['a', '2.', 'a2', 'c']\n","Ph.D. ['phd']\n","Ph. ['ph']\n","D. ['d']\n","''https://hithere.com+https://howdy.org!? ['https', 'hitherecom', 'https', 'howdyorg']\n","https://noheading.com ['https://noheading.com']\n"]}],"source":["import re\n","from string import punctuation\n","\n","def check_url(token):\n","    if re.search(r\"^(https?://)|^(HTTPS?://)\", token):\n","        return True\n","    else:\n","        return False\n","    \n","def check_number(token):\n","    if re.search(r\"[a-zA-Z]\", token):\n","        return False\n","    if re.search(r\"^([+-.,]*\\d+|\\d+[+-.,]*)+([+-.,]*\\d*|\\d*[+-.,])*([+-.,]*\\d*|\\d*[+-.,])+$\", token):\n","        return True\n","    \n","    return False\n","    \n","def is_hyphen(token):\n","    hyphen_bytes = \"-\".encode(\"utf-8\")\n","    for char in token:\n","        decoded_char = char.encode(\"utf-8\")\n","        if hyphen_bytes == decoded_char:\n","            return True\n","    return False\n","\n","\n","def tokenize_fancy(input_token: str) -> list[str]:\n","    \"\"\"\n","    Tokenize ONE token into tokens using the fancy rules defined.\n","\n","    Args:\n","        input_token: the token that we want to tokenize.\n","\n","    Returns: an array of sub-tokens in format,\n","        [\n","            sub_token_1_1,\n","            sub_token_1_2,\n","            ...\n","        ], or\n","        [\n","            'c',\n","            'a',\n","            '2.',\n","            'a2'\n","        ].\n","    \"\"\"\n","    if input_token == \"\":\n","        return []\n","    \n","    if check_url(input_token):\n","        return [input_token.rstrip(punctuation)]  # remove trailing punctuation\n","    \n","    input_token = input_token.lower()\n","\n","    if check_number(input_token):\n","        return [input_token]\n","    \n","    input_token = input_token.replace(\"'\", \"\") # remove abbreviations\n","    final_tokens = []\n","    processed_tokens = []\n","    special_punctuation = punctuation.replace(\"-\", \"\").replace(\".\", \"\")\n","\n","    split_tokens = re.split(f\"[{re.escape(special_punctuation)}]\", input_token) # split on any punctation except period hyphens, words and numbers\n","    tokens = [token for token in split_tokens if token] # remove any empty strings\n","\n","    for tok in tokens:\n","        if is_hyphen(tok) and not check_number(tok):  # its an actual hyphen, and the subtoken is not numbers\n","            hyphen_split = tok.split(\"-\")\n","            for ele in hyphen_split:\n","                final_tokens.extend(tokenize_fancy(ele))   # recurisvely call on subtokens here\n","            final_tokens.append(\"\".join(hyphen_split))\n","        else:\n","            final_tokens.append(tok)\n","\n","    processed_tokens = []\n","    for tok in final_tokens:\n","        if '.' in tok and (re.search(r\"[a-zA-Z]\", tok) and not check_number(tok)) or tok == \".\": # there is letters, there is a \".\" in it, and the subtoken is not a number\n","            sub_tokens = tokenize_fancy(tok.replace(\".\", \"\"))\n","            processed_tokens.extend(sub_tokens)\n","        else:\n","            processed_tokens.append(tok)\n","\n","    return [tok for tok in processed_tokens if tok]\n","sample_sentence = 'a.-2./c. Ph.D. Ph. D. \\'\\'https://hithere.com+https://howdy.org!? https://noheading.com' \n","# Check if the output matches the example mentioned above\n","for token in tokenize_space(sample_sentence):\n","  tokens_from_fancy_tokenization = tokenize_fancy(token)\n","  print(token, tokens_from_fancy_tokenization)\n","got = tokenize_space(\" \".join(tokenize_fancy('\"                                        d=\"M189.95,70.33a11.82,11.82,0,0,0-6.24-7,10.92,10.92,0,0,0-2.32-.52,89,89,0,0,0-44-44.07,10.82,10.82,0,0,0-.56-2.43,11.77,11.77,0,0,0-7-6.29,93.75,93.75,0,0,0-59.5,0,11.71,11.71,0,0,0-7,6.23,10,10,0,0,0-.54,2.32,88.78,88.78,0,0,0-44.06,44,11.62,11.62,0,0,0-2.46.57,11.8,11.8,0,0,0-6.28,7,93.72,93.72,0,0,0,0,59.49,11.74,11.74,0,0,0,6.22,7,11.74,11.74,0,0,0,2.33.51,88.93,88.93,0,0,0,44,44.09,11.59,11.59,0,0,0,.57,2.44,11.8,11.8,0,0,0,7,6.28,93.75,93.75,0,0,0,59.49,0,9.84,9.84,0,0,0,7.51-8.55,88.88,88.88,0,0,0,44.08-44,11.62,11.62,0,0,0,2.45-.54,11.81,11.81,0,0,0,6.27-7A93.79,93.79,0,0,0,189.95,70.33ZM135.12,30a78,78,0,0,1,35,35l-24,9.89a51.74,51.74,0,0,0-20.94-21ZM119,136.8a11.28,11.28,0,0,0-9.37-1.75,36.37,36.37,0,0,1-19.49,0,11.11,11.11,0,0,0-9.31,1.72,40.94,40.94,0,0,1-17.66-17.72,11.22,11.22,0,0,0,1.74-9.39,36.34,36.34,0,0,1,0-19.51,11.13,11.13,0,0,0-1.72-9.3A40.94,40.94,0,0,1,80.81,63.28,11.2,11.2,0,0,0,90.33,65a36.37,36.37,0,0,1,19.49,0,11.15,11.15,0,0,0,9.31-1.72A41,41,0,0,1,136.79,81a11.12,11.12,0,0,0-1.74,9.37,36.37,36.37,0,0,1,0,19.49,11.07,11.07,0,0,0,1.73,9.32A41,41,0,0,1,119,136.8Zm7.3-116.17.38.56-13.88,33a48.8,48.8,0,0,0-13.08-1.76A53.23,53.23,0,0,0,87,54L73.84,20.64A82.58,82.58,0,0,1,126.33,20.63ZM65,29.88l9.9,24a51.93,51.93,0,0,0-21,20.95L30,64.88A77.94,77.94,0,0,1,65,29.88ZM21.18,73.28l33,13.88A50.72,50.72,0,0,0,54,113L20.64,126.15a82.66,82.66,0,0,1,0-52.49ZM64.87,170a77.79,77.79,0,0,1-35-35l24-9.88a52,52,0,0,0,20.94,21Zm8.8,9.39-.39-.56,13.89-33a50.56,50.56,0,0,0,25.8.2l13.18,33.37A82.55,82.55,0,0,1,73.67,179.37Zm61.3-9.26-9.88-24a51.94,51.94,0,0,0,21-20.94L170,135.13A77.78,77.78,0,0,1,135,170.11Zm43.86-43.39-33-13.88A50.83,50.83,0,0,0,146,87l33.37-13.18a82.66,82.66,0,0,1,0,52.49Z\">\"')))\n","# should = ['11.82', '0', '0', '0-6.24-7', '10.92', '10.92', '0', '0', '0-2.32-.52', '89', '89', '0', '0', '0-44-44.07', '10.82', '10.82', '0', '0', '0-.56-2.43', '11.77', '11.77', '0', '0', '0-7-6.29', '93.75', '93.75', '0', '0', '0-59.5', '0', '11.71', '11.71', '0', '0', '0-7', '6.23', '10', '10', '0', '0', '0-.54', '2.32', '88.78', '88.78', '0', '0', '0-44.06', '44', '11.62', '11.62', '0', '0', '0-2.46.57', '11.8', '11.8', '0', '0', '0-6.28', '7', '93.72', '93.72', '0', '0', '0', '0', '59.49', '11.74', '11.74', '0', '0', '0', '6.22', '7', '11.74', '11.74', '0', '0', '0', '2.33.51', '88.93', '88.93', '0', '0', '0', '44', '44.09', '11.59', '11.59', '0', '0', '0', '.57', '2.44', '11.8', '11.8', '0', '0', '0', '7', '6.28', '93.75', '93.75', '0', '0', '0', '59.49', '0', '9.84', '9.84', '0', '0', '0', '7.51-8.55', '88.88', '88.88', '0', '0', '0', '44.08-44', '11.62', '11.62', '0', '0', '0', '2.45-.54', '11.81', '11.81', '0', '0', '0', '93.79', '0', '0', '0', '189.95', '78', '0', '0', '1', '35', '51.74', '0', '0', '11.28', '0', '0', '0-9.37-1.75', '36.37', '36.37', '0', '0', '1-19.49', '0', '11.11', '11.11', '0', '0', '0-9.31', '1.72', '40.94', '40.94', '0', '0', '1-17.66-17.72', '11.22', '11.22', '0', '0', '0', '1.74-9.39', '36.34', '36.34', '0', '0', '1', '0-19.51', '11.13', '11.13', '0', '0', '40.94', '0', '0', '1', '80.81', '63.28', '11.2', '11.2', '0', '0', '0', '90.33', '36.37', '0', '0', '1', '19.49', '0', '11.15', '11.15', '0', '0', '0', '41', '0', '0', '1', '136.79', '11.12', '0', '0', '0-1.74', '9.37', '36.37', '36.37', '0', '0', '1', '0', '19.49', '11.07', '11.07', '0', '0', '0', '1.73', '41', '0', '0', '1', '119', '48.8', '0', '0', '53.23', '0', '0', '0', '87', '82.58', '0', '0', '1', '126.33', '51.93', '0', '0', '0-21', '77.94', '0', '0', '1', '65', '50.72', '0', '0', '0', '54', '82.66', '0', '0', '1', '77.79', '0', '0', '52', '0', '0', '0', '20.94', '9.39-.39-.56', '50.56', '0', '0', '0', '82.55', '0', '0', '1', '73.67', '51.94', '0', '0', '0', '77.78', '0', '0', '1', '135', '50.83', '0', '0', '0', '146', '82.66', '0', '0', '1', '0', 'd', 'm18995', '7033a1182', '7033zm13512', '30a78', '989a5174', '1368a1128', '65a3637', '81a1112', '932a41', '33a488', '54l7384', '2064a8258', '2063zm65', '2988l99', '24a5193', '2095l30', '6488a7794', '2988zm2118', '7328l33', '1388a5072', '113l2064', '12615a8266', '170a7779', '21zm88', '2582l1318', '3337a8255', '13513a7778', '5249z', '6.27', '7a9379', '6277a9379', '35l', '24', '35l24', '0', '20.94', '21zm119', '0209421zm119', '0', '1.72', '93a4094', '017293a4094', '9.31', '172a41', '931172a41', '1368zm73', '116.17.38.56', '13.88', '1368zm731161738561388', '0', '13.08', '176a5323', '01308176a5323', '0', '5249zm6487', '05249zm6487', '1', '35', '35l24', '988a52', '13535l24988a52', '13.89', '33a5056', '138933a5056', '17937zm613', '9.26', '9.88', '24a5194', '17937zm61392698824a5194', '21', '2094l170', '212094l170', '17011zm4386', '43.39', '33', '1388a5083', '17011zm43864339331388a5083', '87l3337', '1318a8266', '87l33371318a8266']\n","# for token in should:\n","#     if token not in got:\n","#         print(token, \"should've got\")\n","# if got.sort() == should.sort(): print(\"yippie\")"]},{"cell_type":"markdown","metadata":{"id":"jCpetrP02vuC"},"source":["Sample Result:\n","\n","| Phrase | Tokens |\n","| :------ | :------ |\n","| a.-2./c. | ['c', 'a', '2.', 'a2'] |\n","| Ph.D. | ['phd' ] |\n","| Ph. | ['ph'] |\n","| D. | ['d'] |\n","| \\'\\'https://hithere.com+https://howdy.org!? | ['https', 'hitherecom', 'https', 'howdyorg'] |\n","| https://noheading.com | ['https://noheading.com'] |"]},{"cell_type":"markdown","metadata":{"id":"riW8cxuITEPZ"},"source":["### 2. Stopping\n","After tokenizing the input file, you will apply a stopword list or not. This function returns a version of the input tokens list with all of the stopwords omitted.\n","\n","#### 2.1 <u>stopping()</u>\n","<u><strong>stopwords=None</strong></u>. means that you should not apply any stopping to the list. The result of this choice means that the token list will be unchanged by the stopping step.\n","\n","\n","<u><strong>stopwords=list[str]</strong></u>. means that you will use the list of words (that was loaded from the stopwords.txt file for you) as stopwords.\n","\n","It doesn't appy in your coding of this function, but note that if an original token results in multiple tokens (spaces or non-period punctuation rules), you must apply the stopword list to <em>each of</em> the resulting tokens.\n","\n","Also note that something that does not look like a stopword in the original text could become one &ndash; consider \"a-n\" in the text that will turn into [\"a\", \"n\", \"an\"] by the hyphen rule that will result in having \"a\" and \"an\" removed (if they're in the stopword list), but the \"n\" will be retained.\n","\n","Those issues should be taken care of in the higher-level user of this function."]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15,"status":"ok","timestamp":1727551242078,"user":{"displayName":"Chang Zeng","userId":"16004812550788429459"},"user_tz":240},"id":"XI4L6TOHTEPZ","outputId":"6eec7c80-03d6-44e5-8cd2-cc824f377140"},"outputs":[{"name":"stdout","output_type":"stream","text":["a.-2./c. ['2.', 'a2', 'c']\n","and []\n","hi ['hi']\n"]}],"source":["def stopping(input_tokens: list[str], stopwords: list[str] = None) -> list[str]:\n","    \"\"\"\n","    Applying stopping to the list of tokens.\n","\n","    Args:\n","        input_tokens: the list of tokens that we want to apply stopwords.\n","        stopwords: the list of stopwords that need to be removed from list of token.\n","            Default to None (an empty list), i.e., do not stop.\n","\n","    Returns: an array of array of tokens in format,\n","        [\n","            sub_token_1_1,\n","            sub_token_1_2,\n","            ...\n","        ], or for example,\n","        [\n","            \"a.-2./c.\",\n","            \"a\",\n","            \"2.\",\n","            \"c\",\n","            \"a2\",\n","            \"c\"\n","        ].\n","    \"\"\"\n","\n","    #########\n","    ##\n","    ## Implement the function here\n","    ##\n","    #########\n","\n","    for stop_word in stopwords:\n","        if stop_word in input_tokens: input_tokens.remove(stop_word)\n","    return input_tokens\n","\n","\n","# Recall that 'stopwords' is the contents of the stopwords file from way before\n","sample_sentence = 'a.-2./c. and hi'\n","for token in tokenize_space(sample_sentence):\n","    stop = stopping(\n","        tokenize_fancy(token), stopwords=stopwords\n","    )\n","    print(token, stop)\n"]},{"cell_type":"markdown","metadata":{"id":"tSv5NBY54o4W"},"source":["Sample Result:\n","\n","| Phrase | Tokens | Tokens After Stopping |\n","| :------ | :------ | :------ |\n","| a.-2./c. | ['c', '2.', 'a', 'a2'] | ['c', '2.', 'a2'] |\n","| and | ['and'] | [] |\n","| hi | ['hi'] | ['hi'] |"]},{"cell_type":"markdown","metadata":{"id":"cgSSgNSR5uK9"},"source":["### 3. Stemming\n","\n","After tokenizing the input file and possibly removing stopwords, you will stem each of the tokens in turn using one of three options. Note that stemming operates on every token that is not stopped, though in many cases (e.g., URLs, numbers, tokens that are already their stem) nothing will change. Also note that if a token generated multiple tokens (the spaces or non-period punctuation rules), the stemmer must be applied to each of them.\n","\n","Both types of stemming accept a list of strings and return a stemmed copy. If the resulting stem is empty (can you see where that can happen?) just return an empty string for that token.\n","\n","Note that we will put the full process together in 4.1 below when the _tokenization()_ function is defined.\n","\n","<u><strong>stemming_type=None</strong></u>. means that you apply no stemming, so the token passes unchanged through this step. We will handle this directly in _tokenization()_ so you do not need to implement it.\n","\n","#### 3.1 \"Suffix s\" stemming\n","\n","<u><strong>stemming_type=suffix_s</strong></u>. means that you apply the \"suffix s\" stemmer, where if the final character of a token is \"s\" you remove the \"s\" and otherwise leave the token unchanged."]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14,"status":"ok","timestamp":1727551242078,"user":{"displayName":"Chang Zeng","userId":"16004812550788429459"},"user_tz":240},"id":"Y_THyjG25v9K","outputId":"e407aa0f-dad4-46ef-d7d9-0da571b4531f"},"outputs":[{"name":"stdout","output_type":"stream","text":["hi ['hi']\n","is ['i']\n","this ['thi']\n","lass ['las']\n","silly ['silly']\n"]}],"source":["import re\n","def stemming_s(input_tokens: list[str]) -> list[str]:\n","    \"\"\"\n","    Applying suffix-s stemming to the list of tokens.\n","\n","    Args:\n","        input_tokens: the list of tokens that we want to perform stemming.\n","\n","    Returns: an array of array of tokens in format,\n","        [\n","            sub_token_1_1,\n","            sub_token_1_2,\n","            ...\n","        ]\n","\n","        For example, if the input is [\"hi\", \"is\", \"this\", \"lass\", \"silly\"] the output should be:\n","        [ \"hi\", \"i\", \"thi\", \"las\", \"silly\" ]\n","    \"\"\"\n","\n","    #########\n","    ##\n","    ## Implement the function here\n","    ##\n","    #########\n","\n","    return [token[0:-1] if token and token[-1] == \"s\" else token for token in input_tokens]\n","\n","\n","sample_sentence = 'hi is this lass silly'\n","for token in tokenize_space(sample_sentence):\n","    stem = stemming_s(tokenize_fancy(token))  # Just tokenizes and skips stopping\n","    print(token, stem)\n"]},{"cell_type":"markdown","metadata":{"id":"siXvYMSB6tZQ"},"source":["Sample Result:\n","\n","| Phrase | Tokens |\n","| :------ | :------ |\n","| hi | ['hi'] |\n","| is | ['i'] |\n","| this | ['thi'] |\n","| lass | ['las'] |\n","| silly | ['silly'] |"]},{"cell_type":"markdown","metadata":{"id":"rtaXXdooTEPZ"},"source":["#### 3.2 Stemming with Porter Stemmer\n","\n","<u><strong>stemming_type=porter</strong></u>. means that you apply the Porter stemmer (we will use \"the English stemmer\" which is also called \"Porter2\"), though you will only implement part of the stemming algorithm. You'll find a description of the Porter2 stemmer in your textbook with Steps 1a and 1b listed. However, please use the following restatement of the algorithm where some of the ambiguities are clarified and where Step 1c is added for fun.\n","\n","For each token in turn, apply the rule of Step 1a that matches the longest suffix (if it matches any) and then apply the one rule of Step 1b that matches the longest suffix (if any) of the output of Step 1a and then apply Step1c to Step 1b's result if it fits. In each step, if no suffix matches, do nothing and then continue to the next step.\n","\n","    Vowels are a, e, i, o, and u.\n","\n","As discussed in class (on Tuesday, September 17), the algorithm in the book talks about several things that are ill-defined as is. To address them, you need a method to determine a type of \"structure\" of the token being stemmed. Here that means figuring out how the token fits the pattern $[C](VC)^m[V]$ where $V$ is a sequence of one or more vowels (as above), C is a sequence of one or more consonents (anything that is not a vowel is a consonent; that includes punctuation), and $m$ indicates how often the $CV$ pattern repeats: if $m=0$ then it does not occur at all. Note that every word can be represented by this pattern. (Note that it's a weird concept for the 4-gram tokenizer. Treat the space as if it is a consonent.)\n","\n","Here are some examples to help you verify you're doing the right thing:\n","\n","* _agreed_ is _VCVC_ which is $(VC)^2$ with the leading $C$ and trailing $V$ empty.\n","* _retrieval_ is _CVCVCVC_ which is $C(VC)^3$ with the trailing $V$ empty.\n","* _feed_ is _CVC_ which is $C(VC)^1$ with the trailing $V$ empty.\n","* _tree_ is _CV_ which is $C(VC)^0V$ or $CV$ with nothing in the middle.\n","\n","And now, on to the steps of the algorithm, reworked slightly from the description in the book for clarity. Recall that each step is looking at the _end_ of the word.\n","\n","Step 1a:\n","  * Replace <em>sses</em> by <em>ss</em> (e.g., <em>stresses&rarr;stress</em>) and do nothing else for Step 1a.\n","  * If the stem ends with <em>ied</em> or <em>ies</em>, then if the remaining stem is _more than one_ character long, replace the ending by _i_, otherwise replace it by _ie_ (e.g., <em>ties&rarr;tie, cries&rarr;cri</em>). In either case, do nothing else for Step 1a.\n","  * If the stem ends in <em>us </em>or <em>ss </em>do nothing (e.g., <em>stress&rarr;stress</em>), including doing nothing else for Step 1a\n","  * If the stem ends with _s_, then \"if the preceding stem part contains a vowel not immediately before the _s_\" then delete the trailing _s_  (e.g., <em>gaps&rarr;gap</em> but <em>gas&rarr;gas</em>). And do nothing else for Step 1a. (Note that the \"contains a vowel\" bit is not helped by the $m$ rule described above. You'll need another way to check for that.)\n","\n","Step 1b:\n","  * If the stem ends in <em>eed </em>or <em>eedly </em><u>then:</u>\n","      * if it is in the part of the stem after the first non-vowel following a vowel (i.e., if $m>1$ in the token that gets to 1b), replace it by <em>ee </em>(e.g., <em>agreed&rarr;agree, feed&rarr;feed</em>).\n","      * then go to step 1c\n","  * If the stem ends in <em>ed, edly, ing</em>, or <em>ingly </em>then:<u><strong> <br /></strong></u>\n","      * <u><strong></strong></u>if the preceding stem part <em>does not</em> contain a vowel, go to step 1c\n","\n","      * if the preceding stem part <em>does</em> contain a vowel delete the ending <u><em>and then also consider the following three possibilities with the resulting stem:</em></u>\n","          * if the stem now ends in <em>at</em>, <em>bl</em>, or <em>iz </em>add _e_ (e.g., <em>fished &rarr; fish, pirating &rarr;pirate</em>) and go to step 1c\n","\n","          * if the stem now ends with a double letter that is one of <em>bb, dd, ff, gg, mm, nn, pp, rr,</em> or <em>tt</em>, remove the last letter (e.g., <em>falling&rarr;fall, dripping&rarr;drip</em>) and go to step 1c\n","\n","          * if the stem is now short (defined as $m=1$ for the stem after removing the suffix), add _e_ (e.g., <em>hoping&rarr;hope</em>) and go to step 1c.\n","             <!-- * Some example short words by this definition are <em>at </em>or <em>ow </em>(or \"<em>or</em>\"!). Surprisingly, <em>be</em> is not short because it is $CV = C(VC)^0V$, so $m=0$ which is not 1.\n","             * These stems are also short: <em>shed, shred, rap, trap, hop</em>, or <em>undo</em>, yet these are not: <em>pfft</em>, <em>eieio</em>, <em>to</em>, or <em>flu</em>). -->\n","\n","Step 1c (not in the textbook):\n","  * If the stem ends in <em>y </em>and the character before the <em>y </em>is neither a vowel nor the first letter of the word, replace the <em>y </em>with an <em>i </em>(e.g., <em>cry&rarr;cri, bi&rarr;bi, bamby&rarr;bambi, bambi&rarr;bambi, say&rarr;say, why&rarr;whi</em>)\n","\n","_Suggestion:_ You may find it useful to create helpful functions that implement each of the steps above. You don't have to, but since they act indepently and each step accepts a token/stem and then emits a new (or the same) token/stem, it might make things cleaner. To be clear: you can do anything you want to implement this, but you must have the function with the indicated arguments and result.\n"]},{"cell_type":"code","execution_count":151,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13,"status":"ok","timestamp":1727551242078,"user":{"displayName":"Chang Zeng","userId":"16004812550788429459"},"user_tz":240},"id":"k8NuAW9GTEPZ","outputId":"4540f807-ac3c-4a1b-bd41-63b70e51cb50"},"outputs":[{"name":"stdout","output_type":"stream","text":["up ['up']\n","is ['is']\n","at ['at']\n","if ['if']\n","on ['on']\n","to ['to']\n","ties ['tie']\n","tied ['tie']\n","died ['die']\n","dies ['die']\n","flies ['fli']\n","cried ['cri']\n","cookies ['cooki']\n","bosses ['boss']\n","misses ['miss']\n","pre-arranged ['pre', 'arrang', 'prearrang']\n","multi-sensored ['multi', 'sensor', 'multisensor']\n","say ['say']\n","play ['play']\n","buy ['buy']\n","joy ['joy']\n","joying ['joye']\n","joyed ['joye']\n","cry ['cri']\n","try ['tri']\n","sitting ['sit']\n","plotting ['plot']\n","felling ['felle']\n","tries ['tri']\n","flies ['fli']\n","untied ['unti']\n","partied ['parti']\n","studies ['studi']\n","buzzing ['buzze']\n","jazzed ['jazze']\n","fizzing ['fizze']\n","bluffing ['bluf']\n","gruff ['gruff']\n","overdoing ['overdo']\n","redoing ['redoe']\n","replaying ['replay']\n","unhappiness ['unhappiness']\n"]}],"source":["import re\n","from string import punctuation\n","\n","def step_1b(token: str) -> str:\n","    new_str = token\n","\n","#   -- start -- coveres steps with -eed or -eedly\n","    if re.search(r\"eed$\", token) and len(re.findall(r\"[aeiou][^aeiou]\", token.lower())) > 1:\n","        return step_1c(token[0:-3] + \"ee\")\n","    \n","    elif re.search(r\"eed$\", token) and len(re.findall(r\"[aeiou][^aeiou]\", token.lower())) <= 1:\n","        return step_1c(token)\n","\n","    elif re.search(r\"eedly$\", token) and len(re.findall(r\"[aeiou][^aeiou]\", token.lower())) > 1:\n","        return step_1c(token[0:-5] + \"ee\")\n","    \n","    elif re.search(r\"eedly$\", token) and len(re.findall(r\"[aeiou][^aeiou]\", token.lower())) <= 1:\n","        return step_1c(token)\n","#   -- end -- \n","\n","# -- start -- covers steps with -ed -edly -ing or -ingly\n","    elif re.search(r\"(ed$|edly$|ingly$|ing$)\", token):\n","        new_str = re.sub(r\"(ed$|edly$|ingly$|ing$)\", \"\", token)\n","        if not re.search(r\"[aeiou]\", new_str.lower()):\n","            return step_1c(token)\n","            \n","        elif re.search(r\"[aeiou]\", new_str.lower()):\n","            if re.search(r\"at$|bl$|iz$\", new_str):\n","                return step_1c(new_str + \"e\")\n","            \n","            elif len(new_str) > 1 and new_str[-1] == new_str[-2] and new_str[-1] in \"bdfgmnprt\":\n","                return step_1c(new_str[0:-1])\n","\n","            elif check_stem_short(new_str):\n","                return step_1c(new_str + \"e\")\n","        \n","    return step_1c(new_str)\n","\n","def step_1a(token: str) -> str:\n","    if re.search(r\"sses$\", token):\n","        return token[0:-4] + \"ss\"\n","    \n","    elif re.search(r\"^.{2,}(ied$|ies$)\", token):\n","        remove_suffix = \"\".join(re.findall(r\"^.{2,}(ied$|ies$)\", token))\n","        return token[0:-len(remove_suffix)] + \"i\"\n","    \n","    elif re.search(r\"^.{1}(ied$|ies$)\", token):\n","        remove_suffix = \"\".join(re.findall(r\"^.(ied$|ies$)\", token))\n","        return token[0:-len(remove_suffix)] + \"ie\"\n","    \n","    elif re.search(r\"ss$|us$\", token):\n","        return token\n","\n","    elif token and token[-1] == \"s\" and re.search(r\"[aeiouAEIOU]+\", token[0:-2]) and (not re.search(r\"^(\\d)\", token) or re.search(r\"^[a-zA-Z](\\d)*[a-zA-Z]\", token)):\n","        return token[0:-1]\n","    \n","    return token\n","\n","def step_1c(input_str: str) -> str:\n","    ret_str = input_str\n","    if re.search(r\"^.+[^aeiouAEIOU]y$\", input_str):\n","        ret_str = input_str[0:-1] + \"i\"\n","    return ret_str\n","\n","def check_stem_short(input_str: str) -> bool:\n","    if re.search(r\"[^aeiou]*[aeiou]*([aeiou][^aeiou]){1}[^aeiou]*[aeiou]*$\", input_str.lower()) and len(re.findall(r\"[aeiou][^aeiou]\", input_str.lower())) == 1:\n","        return True\n","    return False\n","\n","# def copy_and_remove_punctuation(input_str: str) -> tuple[str, str]:\n","#     find_punc = re.findall(f\"[{re.escape(punctuation)}]\", input_str)\n","#     return (input_str.rstrip(\"\".join(find_punc)), \"\".join(find_punc))\n","\n","def stemming_porter(input_tokens: list[str], need_stemming: bool = True) -> list[str]:\n","    \"\"\"\n","    Applying stemming to the list of tokens.\n","\n","    Args:\n","        input_tokens: the list of tokens that we want to apply stopping list\n","        need_stemming: whether we want to apply stem on the token list\n","\n","    Returns: an array of array of tokens in format,\n","        [\n","            sub_token_1_1,\n","            sub_token_1_2,\n","            ...\n","        ], or\n","        [\n","            \"a.-2./c.\",\n","            \"2.\",\n","            \"c\",\n","            \"a2\",\n","            \"c\"\n","        ].\n","    \"\"\"\n","\n","    if need_stemming:\n","        modified_tokens = []\n","        for token in input_tokens:\n","            new_token = step_1a(token)\n","            modified_tokens.append(step_1b(new_token))\n","\n","        return modified_tokens # You will return something before this statement\n","    else:\n","        return input_tokens\n","\n","# sample_sentence = 'a.-2./c. cry pirating gas hopiNG agreeing USD$10-30 ruleS reSultS stress stopwords fishing stemming loving sheded 1Bonus abso-frigging-lutely learning source englishsource described \"Embed designed Meetings examples wileyplus canvas videos. heap-3001039959.js'\n","# sample_sentence = '\"https://www.wiley.com/college/wileyplus/images/resource_discovery_tool_50x50.png\" wileyplus rules examples'\n","\n","\n","# for token in tokenize_space(sample_sentence):\n","#     stem = stemming_porter(\n","#         stopping(tokenize_fancy(token), stopwords=stopwords),\n","#         need_stemming=True\n","#     )\n","#     print(token, stem)\n","\n","# sample_sentence = 'oringly \"Embed tied cries ties blues hOping hOped hopING hopEd multi-tasking pre-setting fizzing fated faded receiving bless ties tied pass mess fried friends cry why say wing feeding rapping'\n","# sample_sentence = 'oringly \"Embed ties tied @!#$%^&*()123hoping cried gas blues falling feed arguing bias species bus'\n","# sample_sentence = 'oringly \"Embed ties tied @!#$%^&*()123hoping 123cried gas blues falling feed arguing bias species bus why try sitting planned gases species bias tidy shed addressing classes atbliz tree ties'\n","# sample_sentence = 'stresses ties cries tied agreed retrieval pirating fished hopping falling dripping running fall hoped sitting plan planed gases gaps values bus species bias agreed seemly anxiously tidy feed tree shed undo trap hop running! !@#$%^&*()values species! ties!? feeding happiness cares !#stressless atbliz stress us gas miss bus less classes addressed'\n","# sample_sentence = '<script type=\"text/javascript\" async=\"\" src=\"446-canvas-page_files/heap-3001039959.js\"></script>'\n","# sample_sentence = '<!DOCTYPE html> <html lang=\"en\"> <head> <meta charset=\"UTF-8\"> <title>Porter Stemmer Test</title> </head> <body><h1>Porter Stemmer Example</h1> <p>This is a test to see how well the Porter stemmer works with HTML content.</p> <p>The Porter stemmer is used to reduce words to their root form.</p><ul> <li>Stemming</li><li>Testing</li> <li>Algorithms</li> </ul> <p>Here\"s a sample sentence: \"Testing the Porter stemming algorithm to see how it performs.\"</p></body></html>'\n","# for token in tokenize_space(\"\".join(train_sentences)):\n","#     stem = stemming_porter(\n","#         stopping(tokenize_fancy(token), stopwords=stopwords),\n","#         need_stemming=True\n","#     )\n","#     print(token, stem)\n","for token in tokenize_space(sample_sentence):\n","    stem = stemming_porter(\n","        stopping(tokenize_fancy(token), stopwords=[]),\n","        need_stemming=True\n","    )\n","    print(token, stem)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"UBdYEejrTEPZ"},"source":["#### 4.1 Tokenization with or without stopping and stemming: <u>tokenization()</u>\n","\n","<u>tokenization()</u> will tokenize a list of sentences given the option specified and return a list of processed token in the required format. Each element of the returned list (a tuple) will contain the original space-separated word and a list of token that are generated using the original space-separated word. So if fancy tokenization, normal stopping, and the Porter stemming are enabled, the sentences\n","\n","    sentence 1: \"whitespace-Separated\" ⍟ tokens ⍟ (as ⍟ in ⍟ P0).\n","    sentence 2: And, ⍟ a.-2./c. ⍟ also.\n","    \n","where \"⍟\" indicates whitespace sequences, would result in\n","\n","| Phrase | Tokens|\n","| :------- | :-------- |\n","|<span class=\"\" style=\"color: #98ca3e;\">\"whitespace-Separated\"| [\"whitespace\", \"separate\", \"whitespaceseparate\"] |\n","|<span class=\"\" style=\"color: #98ca3e;\">tokens| [\"token\"] |\n","|<span class=\"\" style=\"color: #98ca3e;\">(as| [] |\n","|<span class=\"\" style=\"color: #98ca3e;\">in| [] |\n","|<span class=\"\" style=\"color: #98ca3e;\">P0).| [\"p0\"] |\n","|<span class=\"\" style=\"color: #98ca3e;\">And,| [] |\n","|<span class=\"\" style=\"color: #98ca3e;\">a.-2./c.| [\"2.\", \"c\", \"a2\"] |\n","|<span class=\"\" style=\"color: #98ca3e;\">also.| [\"also\"] |\n","\n","\n","Several expectation as follows:\n","* The tokens must be listed in the order that the original tokens were encountered in the input file.\n","* If an original token is removed (e.g., stopped) nothing will be printed for that token (e.g., the line for <span class=\"\" style=\"color: #98ca3e;\">\"(as\", <span class=\"\" style=\"color: #98ca3e;\">\"in\", and <span class=\"\" style=\"color: #98ca3e;\">\"And,\").\n","* If an original token is transformed into additional tokens (e.g., because of hyphens) all of the new tokens will be listed on the same line (e.g., the hyphenated word and the weird <span class=\"\" style=\"color: #98ca3e;\">\"a.-2./c.\" sequence).\n","* If a token is stemmed, the stem will be displayed with the original token. Note that the hyphenated word is separated, made into three tokens, and each of them is stemmed.\n","* Note that the process of tokenization with stopping and stemming can be executed as you read through the input file, though in this assignment we choose to read all the content in the file and then process it.\n","\n","_NOTE:_ We are providing a solution to this function since all it does it call the functions you wrote above. The autograder will use this function, so although you can modify it if you want, be sure to properly handle all of the possible inputs and to produce output of the correct format."]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":188,"status":"ok","timestamp":1727551672326,"user":{"displayName":"Chang Zeng","userId":"16004812550788429459"},"user_tz":240},"id":"lJaSwEYJTEPb","outputId":"ca23a7cc-fb90-48fa-c79f-9f1703a6581f"},"outputs":[{"name":"stdout","output_type":"stream","text":["a.-2./c. ['2.', 'a2', 'c']\n","pirating ['pirate']\n","gas ['gas']\n","and []\n","hoping ['hope']\n","\n"]}],"source":["def tokenization(\n","    input_list_str: list[str],\n","    stopwords: list[str] = None,\n","    tokenizer_type: str = None,\n","    stemming_type: str = None\n",") -> list[tuple[str, list[str]]]:\n","    \"\"\"\n","    Tokenize the input sentences and apply stopping and stemming procedure to the tokenized sentences.\n","\n","    Args:\n","        input_list_str: the list of sentences that we read from the input file.\n","        stopwords: the list of stopwords that need to be removed from list of token (omit to skip stopping)\n","        tokenizer_type: (case-insensitive) the tokenizer method that we want to apply on the token list.\n","              Options are [\"space\", \"4grams\", \"fancy\"].\n","              Default and unrecognized value(s) are considered as \"space\".\n","        stemming_type: (case-insensitive) the stemming method that we want to apply on the token list.\n","              Options are [None, \"suffix_s\", \"porter\"].\n","              Default and unrecognized value(s) are considered as None.\n","\n","    Returns: an array of processed tokens (tokenized then stopped then stemmed as indicated) in required format, i.e.,\n","        [\n","            (\"\\\"whitespace-Separate\\\"\", [\"whitespace\", \"separate\", \"whitespaceseparate\"]),\n","            (\"Tokens\", [\"token\"]),\n","            ...\n","        ].\n","    \"\"\"\n","    tokenizer_method = tokenize_space\n","    tokenizer_per_token_func = lambda x: [x]\n","    if tokenizer_type.lower() == \"4grams\":\n","      tokenizer_method = tokenize_4grams\n","    elif tokenizer_type.lower() == \"fancy\":\n","      tokenizer_per_token_func = tokenize_fancy\n","\n","\n","    stemming_method = lambda x: x # a function that returns its argument\n","    if stemming_type.lower() == \"porter\":\n","      stemming_method = stemming_porter\n","    elif stemming_type.lower() == \"suffix_s\":\n","      stemming_method = stemming_s\n","\n","\n","    recorder = []\n","    for input_str in input_list_str:  # every line\n","        recorder.extend(\n","            [\n","                (\n","                    token,\n","                    stemming_method(\n","                        stopping(tokenizer_per_token_func(token), stopwords=stopwords)),\n","                )\n","                for token in tokenizer_method(input_str)\n","            ]\n","        )\n","\n","    return recorder\n","\n","\n","sample_sentences = ['a.-2./c. pirating', 'gas and hoping']\n","processed_tokens = tokenization(sample_sentences, stopwords=stopwords, tokenizer_type=\"fancy\", stemming_type=\"porter\")\n","for phrase, tokens in processed_tokens:\n","    print(phrase, tokens)\n","print()\n","\n"]},{"cell_type":"markdown","metadata":{"id":"399jkoRn6jhM"},"source":["Sample Result:\n","\n","| Phrase | Tokens |\n","| :------ | :------ |\n","| a.-2./c. | ['c', '2.', 'a2'] |\n","| pirating | ['pirate'] |\n","| gas | ['gas'] |\n","| hoping | ['hope'] |"]},{"cell_type":"markdown","metadata":{"id":"8gFi6e2RTEPb"},"source":["#### 4.2 Token Statistics - <u>heaps()</u>\n","<u>heaps()</u> analyzes the processed tokens and counts the number of unique tokens.\n","\n","Do not count stopwords if they are being removed, but do count the extra words that hyphens and other non-period punctuations generate. The return should have the format:[ (10, 8), (20, 17), ... ], where the first component is the number of tokens you have generated so far and the second component is the number of those tokens so far if you remove duplicates.\n","\n","For both counts, these are the tokens after the process of tokenizing, stopping, and stemming steps (indicated by the options in <u>tokenization()</u>), not before those steps. Clearly the second count will never be greater than the first.\n","\n","To reduce the number of tuples you are generating by 90%, skip any pair where the first number is not a multiple of 10. Except always output the final pair (total number of tokens, total number of unique tokens) even if 10 does not divide it."]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":180,"status":"ok","timestamp":1727551495223,"user":{"displayName":"Chang Zeng","userId":"16004812550788429459"},"user_tz":240},"id":"vr30DxQfTEPb","outputId":"1efa2071-97e2-48d2-85b4-d76552338b67"},"outputs":[{"name":"stdout","output_type":"stream","text":["(10, 10)\n","(20, 19)\n","(30, 27)\n","(40, 33)\n","(50, 42)\n","(60, 52)\n","(70, 60)\n","(80, 68)\n","(90, 77)\n","(100, 85)\n"]}],"source":["def heaps(processed_tokens: list[tuple[str, list[str]]]) -> list[tuple[int, int]]:\n","    \"\"\"\n","    Analyze the processed tokens.\n","\n","    Args:\n","        processed_tokens: the list of processed tokens in the expected format.\n","\n","    Returns: an array of statistics in required format, i.e.,\n","        [\n","            (10, 8),\n","            (20, 17),\n","            ...\n","        ].\n","    \"\"\"\n","\n","    #########\n","    ##\n","    ## Implement the function here\n","    ##\n","    #########\n","    return_lst = []\n","    duplicate_set = set()\n","    count = 0\n","\n","    for untokenized_token, tokens in processed_tokens:\n","        \n","        if tokens: # don't count stop words\n","            count += len(tokens)\n","            for tok in tokens:\n","                if tok not in duplicate_set:\n","                    duplicate_set.add(tok)\n","\n","        if count % 10 == 0 and count != 0 and (count, len(duplicate_set)) not in return_lst:\n","            return_lst.append((count, len(duplicate_set)))\n","    \n","    if (count, len(duplicate_set)) not in return_lst:\n","        return_lst.append((count, len(duplicate_set)))\n","    \n","    return return_lst\n","\n","processed_tokens = tokenization(train_sentences, stopwords=stopwords, tokenizer_type=\"fancy\", stemming_type=\"porter\")\n","heaps_result = heaps(processed_tokens)\n","[print(curr) for curr in heaps_result[:10]];"]},{"cell_type":"markdown","metadata":{"id":"QYzPzR-MTEPb"},"source":["#### 4.3 Token Statistics - <u>statistics()</u>\n","\n","<u>statistics()</u> analyze the statistic of the processed tokens and return the following information in a tuple:\n","    <ul>\n","        <li>the first element is the total number of tokens you encountered</li>\n","        <li>the second is the total number of unique tokens you encountered</li>\n","        <li>the third element is an array contains the 100 most frequent final tokens you generated &ndash; i.e., after all of tokenizing, stopping, and stemming are done. Each element should have the format <code>[token, token_count]</code> where <code>token_count</code> is the number of occurrences of <code>token</code> you found. Sort them in descending order by the value of <code>token_count</code>. If there are multiple words with the same value of <code>token_count</code>, sort them alphabetically by <code>token</code>. If the same value of <code>token_num</code> happens for the 100th and 101st items, just stop at 100 with a tie breaker based on alphabetical order, for example, between (apple, 100) and (banana, 100), the result should be (apple, 100).</li>\n","    </ul>\n","\n","Note that the first total number of tokens and unique tokens should match with the last element of the result of <u>heaps()</u>.\n","</div>"]},{"cell_type":"code","execution_count":152,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":175,"status":"ok","timestamp":1727551498814,"user":{"displayName":"Chang Zeng","userId":"16004812550788429459"},"user_tz":240},"id":"1mWfpcVQTEPc","outputId":"8110c44a-89f5-4baf-9bee-e16540cfb313"},"outputs":[{"name":"stdout","output_type":"stream","text":["Overall Token Counts: 91492; Overall Unique Token Counts: 5819\n","Top 10 frequent tokens with count:\n","('her', 2294)\n","('i', 2121)\n","('she', 1752)\n","('not', 1500)\n","('you', 1375)\n","('his', 1303)\n","('had', 1186)\n","('but', 1041)\n","('have', 974)\n","('mr', 809)\n","('him', 777)\n","('my', 728)\n","('all', 652)\n","('elizabeth', 645)\n","('they', 610)\n","('so', 607)\n","('which', 569)\n","('been', 536)\n","('could', 531)\n","('no', 511)\n","('veri', 505)\n","('what', 494)\n","('would', 485)\n","('your', 478)\n","('this', 466)\n","('me', 462)\n","('their', 459)\n","('them', 445)\n","('darci', 432)\n","('will', 428)\n","('said', 406)\n","('such', 397)\n","('when', 389)\n","('do', 381)\n","('there', 373)\n","('if', 367)\n","('mrs', 354)\n","('bennet', 350)\n","('more', 337)\n","('much', 337)\n","('or', 327)\n","('am', 325)\n","('bingley', 316)\n","('miss', 315)\n","('must', 315)\n","('one', 304)\n","('jane', 302)\n","('than', 300)\n","('who', 298)\n","('sister', 294)\n","('ani', 283)\n","('other', 283)\n","('ladi', 281)\n","('did', 280)\n","('we', 268)\n","('should', 258)\n","('know', 254)\n","('though', 239)\n","('herself', 236)\n","('how', 235)\n","('never', 230)\n","('well', 230)\n","('before', 228)\n","('onli', 226)\n","('think', 226)\n","('time', 223)\n","('can', 218)\n","('see', 218)\n","('soon', 217)\n","('make', 216)\n","('now', 215)\n","('some', 215)\n","('good', 209)\n","('may', 207)\n","('might', 207)\n","('after', 205)\n","('wickham', 200)\n","('most', 199)\n","('own', 197)\n","('little', 192)\n","('nothe', 188)\n","('everi', 185)\n","('again', 182)\n","('be', 182)\n","('day', 180)\n","('come', 177)\n","('then', 177)\n","('without', 177)\n","('lydia', 176)\n","('say', 175)\n","('even', 173)\n","('hope', 172)\n","('friend', 170)\n","('collin', 167)\n","('illustration', 167)\n","('go', 164)\n","('dear', 163)\n","('give', 163)\n","('room', 163)\n","('shall', 163)\n"]}],"source":["import heapq\n","\n","def statistics(\n","    processed_tokens: list[tuple[str, list[str]]], top_k: int = 100\n",") -> tuple[int, int, list[tuple[str, int]]]:\n","    \"\"\"\n","    Analyze the processed tokens.\n","\n","    Args:\n","        processed_tokens: the list of processed tokens in the expected format.\n","\n","    Returns:\n","        total number of tokens encountered;\n","        total number of unique tokens encountered;\n","        100 most frequent processed tokens encountered in required format, i.e.,\n","        [\n","            (\"Test\", 10),\n","            (\"Token\", 10),\n","            (\"Apple\", 9),\n","            ...\n","        ].\n","    \"\"\"\n","    total_token_count = 0\n","    unique_token_count = 0\n","    duplicate_tokens = set()\n","    count_each_token = []\n","\n","    for untokenized_token, tokens in processed_tokens:\n","\n","        if tokens:\n","            total_token_count += len(tokens)    \n","            for token in tokens:\n","                if token not in duplicate_tokens:\n","                    unique_token_count += 1\n","                    duplicate_tokens.add(token)\n","                    count_each_token.append([token, 1])\n","\n","                elif token in duplicate_tokens:\n","                    for tok_tup in count_each_token:\n","                        if tok_tup[0] == token:\n","                            tok_tup[1] = tok_tup[1] + 1\n","    \n","    count_each_token.sort(key=lambda tok_tup: tok_tup[0])\n","    count_each_token.sort(reverse=True, key=lambda tok_tup: tok_tup[1])\n","\n","    #########\n","    ##\n","    ## Implement the function here\n","    ##\n","    #########\n","\n","    return (total_token_count, unique_token_count, [(token, count) for token, count in count_each_token[0:100]])  # You will return something before this statement\n","\n","processed_tokens = tokenization(sentences, stopwords=stopwords, tokenizer_type=\"fancy\", stemming_type=\"porter\")\n","stats_result = statistics(processed_tokens, top_k=100)\n","print(f\"Overall Token Counts: {stats_result[0]}; Overall Unique Token Counts: {stats_result[1]}\")\n","print(\"Top 10 frequent tokens with count:\")\n","[print(curr) for curr in stats_result[2][:100]];"]},{"cell_type":"markdown","metadata":{"id":"TbayZj2MTEPc"},"source":["### 5. Analysis questions"]},{"cell_type":"markdown","metadata":{"id":"EqNWkFobTEPc"},"source":["#### 5.1\n","Run your code with fancy tokenization, stopping, and Porter stemming on <em>PandP.gz</em> and look at the statistics() output to see the most frequent terms from _Pride and Prejudice_. Print the most frequent 100 words and their counts out, sorted in descending order by count). Break ties around the 100th word by alphabetical order of the tokens. You can print them on one big line as long as the full list is visible, or you can print several of them per line.\n","\n","After the list of terms, explain how the top terms are or are not relevant to the story or whether they seem like everyday words that aren't particularly related to the novel? Support your answer with examples (this is required). You may find it useful to skim the <a class=\"\" href=\"https://en.wikipedia.org/wiki/Pride_and_Prejudice\">summary on Wikipedia</a> to know what words are part of the story."]},{"cell_type":"markdown","metadata":{"id":"CtavRPtPTEPc"},"source":["The top terms are relevant to the story, specifically the references to main characters like ('elizabeth', 645), ('darci', 432), and ('collin', 167) for example. In addition, the references to the main characters include the use of mr and mrs, ('mr', 809) and ('mrs', 354), showing the formalilty being used between the characters. Though there are a lot of words used like \"connectors\", there are enough terms that are used frequently such as the main characters, supporting that theres more words related to the novel rather than just being 'everyday' words (though not to discredit, there are still a lot of 'everyday' words)."]},{"cell_type":"markdown","metadata":{"id":"nc37Mlf2TEPc"},"source":["#### 5.2\n","For this question, copy those same 100 words. Then say whether any of those top terms should have been stopwords (in your opinion)? Do the top terms or the list of all tokens suggest any changes to the tokenizing or stemming rules? What are they and why should they be made?"]},{"cell_type":"markdown","metadata":{"id":"2Ty8OFt9TEPc"},"source":["    Enter your answer here..."]},{"cell_type":"markdown","metadata":{"id":"Wl6LdWcWTEPd"},"source":["#### 5.3 Graph Comparison between result and Heaps' law - <u>graph_comparison()</u>\n","\n","Figure 4.4. in the textbook (p. 82) displays a graph of vocabulary growth for the TREC GOV2 collection. The function <u>graph_comparison()</u> below should generate a similar graph which comparing the difference between the results of processed documents (from <u>heaps()</u>) and that of Heaps' Law. Try out some _K_ and _B_ values such that the produced line approximates the line comes from the dataset. This starts out considering what happens with the training data file.\n","\n","_NOTE:_ We are providing the code for the graph generation. You will need to play around with _K_ and _B_ as just mentioned, but the graph generation is not something you need to figure out."]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":472},"executionInfo":{"elapsed":410,"status":"ok","timestamp":1727551516447,"user":{"displayName":"Chang Zeng","userId":"16004812550788429459"},"user_tz":240},"id":"mSQv9YyOTEPd","outputId":"c7575982-47cf-4a03-cd7c-6269f9394da1"},"outputs":[{"ename":"ModuleNotFoundError","evalue":"No module named 'matplotlib'","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","Cell \u001b[1;32mIn[14], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgraph_comparison\u001b[39m(heaps_result: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mint\u001b[39m]], K: \u001b[38;5;28mfloat\u001b[39m, B: \u001b[38;5;28mfloat\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n","\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"]}],"source":["import matplotlib.pyplot as plt\n","import numpy as np\n","\n","\n","def graph_comparison(heaps_result: list[tuple[int, int]], K: float, B: float) -> None:\n","    \"\"\"\n","    Generates a graph that compares the results of processed documents and that of heaps' law.\n","    The curvature of heaps' law is computed using the function f(x) = K*x^B.\n","\n","    Args:\n","        heaps_result: output from the heaps() function\n","        K: heaps' law parameter,\n","        B: heaps' law parameter.\n","    \"\"\"\n","    heaps = np.array(heaps_result)\n","\n","    X = heaps[:,0]\n","    dataset_y = heaps[:,1]\n","    heaps_y = [K * (elem ** B) for elem in X]\n","\n","    _, ax = plt.subplots(1)\n","    ax.plot(X, heaps_y, \"--\", label=f\"Heaps {K} {B}\")\n","    ax.plot(X, dataset_y, label=\"Dataset\")\n","    ax.set_title(\"Vocabulary growth for the Dataset\")\n","    ax.set_xlabel(\"Words in Collection\")\n","    ax.set_ylabel(\"Words in Vocabulary\")\n","    ax.set_xlim(xmin=0)\n","    ax.set_ylim(ymin=0)\n","    plt.legend()\n","    plt.show()\n","\n","\n","\n","heaps_stuff = heaps(tokenization(train_sentences, stopwords=stopwords, tokenizer_type=\"fancy\", stemming_type=\"porter\"))\n","\n","K = 4.0\n","B = 0.76\n","graph_comparison(heaps_result=heaps_stuff, K=K, B=B)"]},{"cell_type":"markdown","metadata":{"id":"nb8kY2mBTEPd"},"source":["(5.3 continued)\n","\n","Now go back and change the heaps_stuff so they are coming from the _Price and Prejudice_ data. You'll have to try different K and B values to see if you can find a fit. Do you feel that _Pride and Prejudice_ follows **Heaps' Law**? Why do you think so or think not? How did you decide on values of B and K?\n","\n","Note: when selecting pages for the question, make sure to include the graph along with your answer."]},{"cell_type":"markdown","metadata":{"id":"J8Jgvz5yTEPd"},"source":["    Enter your answer here\n"]},{"cell_type":"markdown","metadata":{"id":"EIFTTw6AKpvd"},"source":["### 6. Extra credit\n","\n","The following is an extra credit option for this assignment. Success on this part will possibly increase your score on P1. If you submit extra credit that does not work at all, you will lose one point on P1!\n","\n","Your job for this option is to create a plot of how rank and occurrence counts relate to each other. To do generate the graph, it is probably easiest to copy the body of the graph_comparison() function above and edit it to handle Zift-related data and produce a Zipf graph.\n","\n","Similar to the way the Heaps graph showed the K/B graph, you should show the ideal Zipf curve, too -- i.e., what the data would look like if it perfectly followed that law. Note that $c$ is a parameter to the function and a parameter of Zipf's Law."]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":472},"executionInfo":{"elapsed":602,"status":"ok","timestamp":1727551520476,"user":{"displayName":"Chang Zeng","userId":"16004812550788429459"},"user_tz":240},"id":"Ypb7PkfkShPI","outputId":"ace1b4bc-6f04-497d-81f2-e09bc22cd353"},"outputs":[],"source":["\n","def zipf_graph_comparison(stats_info: list[tuple[str, int]], c: float = None) -> None:\n","    \"\"\"\n","    Generates a graph that compares the results of processed documents and that of zipf's law.\n","    The curvature of zipf's law is computed using the function f(x) = c/x, where x is the rank.\n","\n","    Args:\n","        stats_info: the outputs from the statistics() function\n","        c: zipf's law parameter. Using rank 1's probability if not specified.\n","    \"\"\"\n","\n","    #########\n","    ##\n","    ## Implement the function here\n","    ##\n","    #########\n","\n","stats_numbers = statistics(tokenization(train_sentences, stopwords=stopwords, tokenizer_type=\"fancy\", stemming_type=\"porter\"))\n","\n","zipf_graph_comparison(stats_numbers)"]},{"cell_type":"markdown","metadata":{"id":"fvnNu9_1Sv91"},"source":["\n","Consider the graph generated above. Then say whether you feel that _Pride and Prejudice_ follows **Zipf's law**? Why do you think so or think not? Discuss your idea with some reasoning.\n","\n","Note: when selecting pages for the question, make sure to include the graph along with your answer."]},{"cell_type":"markdown","metadata":{"id":"yfA0UGRcTGIU"},"source":["    Please enter your analysis of Zipf's Law here"]},{"cell_type":"markdown","metadata":{"id":"92M-bw8hTEPd"},"source":["### 7. Misc & Grading\n","\n","\n","**SUBMISSION INSTRUCTIONS**. P1 will be graded in two assignments within Gradescope. To start, be sure that your name and ID number and the URL for your notebook are listed at the top of your copy of the notebook.\n","\n","1. The first, called P1, is the autograded part that will exercise your code, running a number of tests on the data you already have and then some tests on held-out data that you do not have access to. Go to gradescope and upload your notebook there.\n","\n","1. The second,  called P1-analysis, is just for the various analysis questions. First make sure that your notebook has run all of the cells and that the output -- the open response questions toward the end and the graphs -- are visible. Then generate a PDF version of your notebook and upload the PDF to P1-analysis. (We'll post a reminder about how to generate the PDF to Piazza; you did it for X1 in case that helps remind you.) In this case, align your notebook to the analysis questions that are provided. Be sure that you include all of the pages that contain a particular question's answer and only pages that contain that particular question's answer.\n","\n","Probably you'll repeat the first part until you're happy with it and then upload the second part when that's all done.\n","\n","Your grade will be split into two parts, one from P1 and one from P1-analysis. They should add to 100 or 110 if you do the extra credit (and get full marks).\n","\n","\n","When you submit your code, the autograder will run:\n","\n","* up to 18 possible combinations of the command line on the training input (3 tokenizers, 2 stemmers + not stemming, stopping or not)\n","\n","* at least spaces-noStop-noStem and fancy-yesStop-porterStem on _Pride and Prejudice_\n","\n","* at least fancy-yesStop-porterStem on a held-out evaluation set that you do not have\n","\n","You may find a utility such as <em>diff</em> (Linux) helpful to compare your output to the expected output on <em>P1-train</em>, though it will require that write the output to a file. For the others, you'll have to eyeball it to see if it looks right. Or you may want to write a quick comparison function for your own use.\n","\n","Some things that you might want to consider while coding and debugging:\n","\n","* If you start with the <em>spaces</em> tokenizer and turn off stopping and stemming, you can check that this foundation step (breaking the text into space-separated tokens) works by a fairly simple examination. That happens to be the same as the spaces-noStop-noStem sample output for P1-train.\n","\n","* Running the output of spaces tokenization through the stopping and stemming algorithms isn't likely to be very helpful for you (though we will grade it!) because the leftover punctuation reduces the number of times stemming occurs.\n","\n","* You can compare spaces-noStop-noStem to fancy-noStop-noStem to see if your fancier tokenizer is working. You can look to see that numbers and URLs pass through unchanged, the hyphens and other punctuation behave as described, and so on.\n","\n","* Then shift to fancy-yesStop-noStem to see if stopping is working\n","\n","* And then fancy-yesStop-yesStem for see if the whole process works\n","\n","* For this project, it does not matter if your code prints debugging information. It will be ignored by the autograder. The exceptions are around the analysis questions: try to keep those free of \"clutter.\""]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.7"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":0}
